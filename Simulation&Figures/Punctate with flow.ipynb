{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for one episode\n",
    "# the agent gets reward at the goal state(=State 10)\n",
    "def punc_f1(gamma, alpha, state_n, init_v, init_q, state_list, action_list, RPE_list, q_RPE_list, \n",
    "            qraw_RPE_list, stay_prob, kappa, QorS): # QorS: 0=Q-learning, 1=SARSA\n",
    "    trial = 1\n",
    "    current_state = 0\n",
    "    trial_list = []\n",
    "    v_state = init_v\n",
    "    v_state = np.array(v_state)\n",
    "    q = init_q\n",
    "    q = np.array(q)\n",
    "    not_end = True\n",
    "    \n",
    "    while not_end:\n",
    "        if current_state == state_n:\n",
    "            not_end = False\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            # Determine the next state and action\n",
    "            if current_state == state_n - 1: # at the goal\n",
    "                next_state = current_state + 1\n",
    "                action = 1 # Go only\n",
    "            \n",
    "            else:\n",
    "                if rd.random() < stay_prob:\n",
    "                    next_state = current_state\n",
    "                    action = 0 # No-Go\n",
    "                else:\n",
    "                    next_state = current_state + 1\n",
    "                    action = 1 # Go\n",
    "            \n",
    "            # Calculate RPE\n",
    "            if current_state == state_n - 1: # at the goal state\n",
    "                reward = 1\n",
    "                delta = reward + 0 - v_state[current_state]\n",
    "                \n",
    "                if QorS == 0: # Q-learning\n",
    "                    # raw RPE of Q learning system\n",
    "                    delta_q = reward + 0 - q[previous_state][previous_action]\n",
    "                \n",
    "                else: # SARSA\n",
    "                    delta_q = reward + 0 - q[previous_state][previous_action]\n",
    "                \n",
    "            else: # at states other than the goal\n",
    "                reward = 0\n",
    "                delta = reward + gamma * v_state[next_state] - v_state[current_state]\n",
    "                \n",
    "                if QorS == 0: # Q-learning\n",
    "                    if trial == 1: # at the first trial\n",
    "                        # raw RPE of Q learning system\n",
    "                        delta_q = reward + gamma * max(q[current_state]) - 0\n",
    "                    else:\n",
    "                        # raw RPE of Q learning system\n",
    "                        delta_q = reward + gamma * max(q[current_state]) - q[previous_state][previous_action]\n",
    "                \n",
    "                else: # SARSA\n",
    "                    if trial == 1: # at the first trial\n",
    "                        delta_q = reward + gamma * q[current_state][action] - 0\n",
    "                    else:\n",
    "                        delta_q = reward + gamma * q[current_state][action] - q[previous_state][previous_action]\n",
    "                        \n",
    "            rpe_with_flow = kappa*delta + (1-kappa)*delta_q\n",
    "                \n",
    "            # Update state values and Q values\n",
    "            v_state[current_state] += alpha * delta\n",
    "            if trial > 1:\n",
    "                q[previous_state][previous_action] += alpha * rpe_with_flow\n",
    "            \n",
    "            state_num = current_state + 1\n",
    "            state_list.append(state_num)\n",
    "            if action == 0:\n",
    "                action_list.append(\"No-Go\")\n",
    "            else:\n",
    "                action_list.append(\"Go\")\n",
    "            RPE_list.append(delta)\n",
    "            q_RPE_list.append(rpe_with_flow)\n",
    "            qraw_RPE_list.append(delta_q)\n",
    "            trial_list.append(trial)\n",
    "            \n",
    "            # Move to the next state\n",
    "            previous_state = current_state\n",
    "            previous_action = action\n",
    "            current_state = next_state\n",
    "            \n",
    "            trial += 1\n",
    "\n",
    "    return v_state, q, state_list, action_list, RPE_list, q_RPE_list, qraw_RPE_list, trial_list\n",
    "\n",
    "# function for multi episodes\n",
    "def punc_f2(epi_num, gamma, alpha, state_n, init_v, init_q, state_list, action_list, RPE_list, \n",
    "          q_RPE_list, qraw_RPE_list, epi_num_list, stay_prob, kappa, QorS):\n",
    "    epi_length = []\n",
    "    q_list = []\n",
    "    \n",
    "    for k in range(epi_num):\n",
    "        c_v_state, c_q, c_state_list, c_action_list, c_RPE_list, c_q_RPE_list, c_qraw_RPE_list, trial_list = \\\n",
    "        punc_f1(gamma, alpha, state_n, init_v, init_q, state_list, action_list, RPE_list, q_RPE_list, \n",
    "                qraw_RPE_list, stay_prob, kappa, QorS)\n",
    "        \n",
    "        for j in range(len(trial_list)):\n",
    "            epi_num_list.append(k+1)\n",
    "                \n",
    "        for j in range(len(trial_list)):\n",
    "            epi_length.append(k+1)\n",
    "        \n",
    "        q_as_list = c_q.tolist()\n",
    "        q_list.append(q_as_list)\n",
    "        \n",
    "        init_v = c_v_state\n",
    "        init_q = c_q\n",
    "        state_list = c_state_list\n",
    "        action_list = c_action_list\n",
    "        RPE_list = c_RPE_list\n",
    "        qraw_RPE_list = c_qraw_RPE_list\n",
    "        q_RPE_list = c_q_RPE_list\n",
    "        \n",
    "    return c_state_list, c_action_list, c_RPE_list, c_q_RPE_list, c_qraw_RPE_list, epi_num_list, epi_length, q_list\n",
    "\n",
    "\n",
    "# function for multi simulations\n",
    "def punc_f3(sim_num, epi_num, gamma, alpha, state_n, state_list, action_list, \n",
    "          RPE_list, q_RPE_list, qraw_RPE_list, epi_num_list, stay_prob, kappa, QorS):\n",
    "    sim_num_list = []\n",
    "    q_list_l = []\n",
    "    \n",
    "    for t in range(sim_num):\n",
    "        # initialize state values and Q values\n",
    "        init_v = []\n",
    "        init_q = []\n",
    "        \n",
    "        for k in range(state_n):\n",
    "            init_v.append(gamma**(state_n - k - 1)) # v_state = [gamma^n-1, gamma^n-2, ..., gamma, 1]\n",
    "        \n",
    "        for k in range(state_n - 1):\n",
    "            init_q.append([gamma**(state_n - k - 1), gamma**(state_n - k - 2)])\n",
    "        \n",
    "        c_state_list, c_action_list, c_RPE_list, c_q_RPE_list, c_qraw_RPE_list, \\\n",
    "        c_epi_num_list, epi_length, q_list = \\\n",
    "        punc_f2(epi_num, gamma, alpha, state_n, init_v, init_q, state_list, action_list,\n",
    "                RPE_list, q_RPE_list, qraw_RPE_list, epi_num_list, stay_prob, kappa, QorS)\n",
    "        \n",
    "        for u in range(len(epi_length)):\n",
    "            sim_num_list.append(t+1)\n",
    "        \n",
    "        q_list_l.append(q_list)\n",
    "        \n",
    "        state_list = c_state_list\n",
    "        action_list = c_action_list\n",
    "        RPE_list = c_RPE_list\n",
    "        q_RPE_list = c_q_RPE_list\n",
    "        qraw_RPE_list = c_qraw_RPE_list\n",
    "        epi_num_list = c_epi_num_list\n",
    "    \n",
    "    return c_state_list, c_action_list, c_RPE_list, c_q_RPE_list, c_qraw_RPE_list, c_epi_num_list, sim_num_list, q_list_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with different parameters\n",
    "# and create dataframe + convert it to csv\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "rd.seed(20201119)\n",
    "\n",
    "for QorS in [0, 1]:\n",
    "    for kappa in [0.0, 0.20, 0.40]:\n",
    "        # set fixed parameters\n",
    "        sim_num = 100\n",
    "        epi_num = 200\n",
    "        gamma = 0.97\n",
    "        alpha = 0.50\n",
    "        state_n = 10\n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        RPE_list = []\n",
    "        q_RPE_list = []\n",
    "        qraw_RPE_list = []\n",
    "        epi_num_list = []\n",
    "        stay_prob = 0.75\n",
    "    \n",
    "        # Simulation\n",
    "        rl = punc_f3(sim_num, epi_num, gamma, alpha, state_n, state_list, action_list, \n",
    "                        RPE_list,q_RPE_list, qraw_RPE_list, epi_num_list, stay_prob, kappa, QorS)\n",
    "    \n",
    "        # Create dataframe\n",
    "        punc_f_res = \\\n",
    "        pd.DataFrame({'Simulation': rl[6], 'Episode': rl[5], 'State': rl[0], 'Action': rl[1], \n",
    "                      'RPE':rl[2], 'Q_RPE':rl[3], 'Q_RPE_raw':rl[4]})\n",
    "\n",
    "        # convert dataframe to csv\n",
    "        if QorS == 0:\n",
    "            punc_f_res.to_csv('./Punctate_flow/Q_g{:.0f}_s{:.0f}_kappa{:.0f}_{:.0f}states.csv'.format(100*gamma, 100*stay_prob, 100*kappa, state_n))\n",
    "            \n",
    "        else:\n",
    "            punc_f_res.to_csv('./Punctate_flow/SARSA_g{:.0f}_s{:.0f}_kappa{:.0f}_{:.0f}states.csv'.format(100*gamma, 100*stay_prob, 100*kappa, state_n))\n",
    "    \n",
    "        # Create dataframe for Q values\n",
    "        q_list = rl[7]\n",
    "        sim_list = []\n",
    "        epi_list = []\n",
    "        state_list = []\n",
    "        q_go = []\n",
    "        q_stay = []\n",
    "\n",
    "        for sim in range(sim_num):\n",
    "            for epi in range(epi_num):\n",
    "                for state in range(state_n - 1):\n",
    "                    qs = q_list[sim][epi][state]\n",
    "                    sim_list.append(sim+1)\n",
    "                    epi_list.append(epi+1)\n",
    "                    state_list.append(state+1)\n",
    "                    q_go.append(qs[1])\n",
    "                    q_stay.append(qs[0])\n",
    "            \n",
    "        q_values = \\\n",
    "        pd.DataFrame({'Simulation': sim_list, 'Episode': epi_list, 'State': state_list, 'Q_go': q_go, 'Q_stay': q_stay})\n",
    "\n",
    "        # convert dataframe to csv\n",
    "        if QorS == 0:\n",
    "            q_values.to_csv('./Punctate_flow/Q_Qvalues_g{:.0f}_s{:.0f}_kappa{:.0f}_{:.0f}states.csv'.format(100*gamma, 100*stay_prob, 100*kappa, state_n))\n",
    "            \n",
    "        else:\n",
    "            q_values.to_csv('./Punctate_flow/SARSA_Qvalues_g{:.0f}_s{:.0f}_kappa{:.0f}_{:.0f}states.csv'.format(100*gamma, 100*stay_prob, 100*kappa, state_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
