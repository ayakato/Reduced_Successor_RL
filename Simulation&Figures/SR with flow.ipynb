{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for one episode\n",
    "# the agent gets reward at the goal state(=State 10)\n",
    "def sr_q1(gamma, alpha, alpha_sr, state_n, init_feat, init_weight, init_q, state_list, \\\n",
    "          action_list, RPE_list, q_RPE_list, qraw_RPE_list, weight_list, stay_prob, kappa, QorS): # QorS: 0=Q-learning, 1=SARSA\n",
    "    weight = init_weight\n",
    "    time_step = 1\n",
    "    feat = init_feat # feature vector; Successor Representation\n",
    "    feat = np.array(feat)\n",
    "    v_state = weight * feat\n",
    "    current_state = 0\n",
    "    timestep_list = []\n",
    "    q = init_q # Q values\n",
    "    q = np.array(q)\n",
    "    not_end = True\n",
    "    \n",
    "    while not_end:\n",
    "        if current_state == state_n:\n",
    "            not_end = False\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            # Determine the next state and action\n",
    "            if current_state == state_n - 1: # at the goal\n",
    "                next_state = current_state + 1\n",
    "                action = 1 # Go only\n",
    "            \n",
    "            else:\n",
    "                if rd.random() < stay_prob:\n",
    "                    next_state = current_state\n",
    "                    action = 0 # No-Go\n",
    "                else:\n",
    "                    next_state = current_state + 1\n",
    "                    action = 1 # Go\n",
    "            \n",
    "            # Calculate RPE\n",
    "            if current_state == state_n - 1: # at the goal state\n",
    "                reward = 1\n",
    "                delta = reward + 0 - v_state[current_state]\n",
    "                delta_sr = 1 + 0 - feat[current_state]\n",
    "                \n",
    "                if QorS == 0: # Q-learning\n",
    "                    # raw RPE of Q learning system\n",
    "                    delta_q = reward + 0 - q[previous_state][previous_action]\n",
    "                \n",
    "                else: # SARSA\n",
    "                    delta_q = reward + 0 - q[previous_state][previous_action]\n",
    "                \n",
    "            else: # at states other than the goal\n",
    "                reward = 0\n",
    "                delta = reward + gamma * v_state[next_state] - v_state[current_state]\n",
    "                delta_sr = 0 + gamma * feat[next_state] - feat[current_state]\n",
    "                \n",
    "                if QorS == 0: # Q-learning\n",
    "                    if time_step == 1: # at the first time-step\n",
    "                        # raw RPE of Q learning system\n",
    "                        delta_q = reward + gamma * max(q[current_state]) - 0\n",
    "                    else:\n",
    "                        # raw RPE of Q learning system\n",
    "                        delta_q = reward + gamma * max(q[current_state]) - q[previous_state][previous_action]\n",
    "                \n",
    "                else: # SARSA\n",
    "                    if time_step == 1: # at the first time-step\n",
    "                        delta_q = reward + gamma * q[current_state][action] - 0\n",
    "                    else:\n",
    "                        delta_q = reward + gamma * q[current_state][action] - q[previous_state][previous_action]\n",
    "            \n",
    "            rpe_with_flow = kappa*delta + (1-kappa)*delta_q\n",
    "                \n",
    "            # Update weights, state values, Q values, and feature\n",
    "            feat[current_state] += alpha_sr * delta_sr\n",
    "            weight += alpha * delta * feat[current_state]\n",
    "            v_state = feat * weight\n",
    "            \n",
    "            if time_step > 1:\n",
    "                q[previous_state][previous_action] += alpha * rpe_with_flow\n",
    "            \n",
    "            state_num = current_state + 1\n",
    "            state_list.append(state_num)\n",
    "            if action == 0:\n",
    "                action_list.append(\"No-Go\")\n",
    "            else:\n",
    "                action_list.append(\"Go\")\n",
    "            RPE_list.append(delta)\n",
    "            q_RPE_list.append(rpe_with_flow)\n",
    "            qraw_RPE_list.append(delta_q)\n",
    "            timestep_list.append(time_step)\n",
    "            weight_list.append(weight)\n",
    "            \n",
    "            # Move to the next state\n",
    "            previous_state = current_state\n",
    "            previous_action = action\n",
    "            current_state = next_state\n",
    "            \n",
    "            time_step += 1\n",
    "\n",
    "    return weight, feat, q, state_list, action_list, RPE_list, q_RPE_list, qraw_RPE_list, timestep_list, weight_list\n",
    "\n",
    "# function for multi episodes\n",
    "def sr_q2(epi_num, gamma, alpha, alpha_sr, state_n, init_feat, feat_list, init_weight, \\\n",
    "          init_q, state_list, action_list, RPE_list, q_RPE_list, qraw_RPE_list, weight_list, epi_num_list, stay_prob, kappa, QorS):\n",
    "    epi_length = []\n",
    "    q_list = []\n",
    "    \n",
    "    for k in range(epi_num):\n",
    "        c_weight, c_feat, c_q, c_state_list, c_action_list, c_RPE_list, c_q_RPE_list, c_qraw_RPE_list, timestep_list, c_weight_list = \\\n",
    "        sr_q1(gamma, alpha, alpha_sr, state_n, init_feat, init_weight, init_q, state_list,\n",
    "              action_list, RPE_list, q_RPE_list, qraw_RPE_list, weight_list, stay_prob, kappa, QorS)\n",
    "        \n",
    "        for j in range(len(timestep_list)):\n",
    "            epi_num_list.append(k+1)\n",
    "                \n",
    "        for j in range(len(timestep_list)):\n",
    "            epi_length.append(k+1)\n",
    "        \n",
    "        feat_list.append(c_feat)\n",
    "        q_as_list = c_q.tolist()\n",
    "        q_list.append(q_as_list)\n",
    "        \n",
    "        init_feat = c_feat\n",
    "        init_weight = c_weight\n",
    "        init_q = c_q\n",
    "        state_list = c_state_list\n",
    "        action_list = c_action_list\n",
    "        RPE_list = c_RPE_list\n",
    "        qraw_RPE_list = c_qraw_RPE_list\n",
    "        q_RPE_list = c_q_RPE_list\n",
    "        weight_list = c_weight_list\n",
    "        \n",
    "    return c_weight, c_feat, feat_list, c_state_list, c_action_list, c_RPE_list, c_q_RPE_list, c_qraw_RPE_list, c_weight_list, epi_num_list, epi_length, q_list\n",
    "\n",
    "\n",
    "# function for multi simulations\n",
    "def sr_q3(sim_num, epi_num, gamma, alpha, alpha_sr, state_n, feat_list, state_list, action_list, \n",
    "          RPE_list, q_RPE_list, qraw_RPE_list, weight_list, epi_num_list, stay_prob, kappa, QorS):\n",
    "    sim_num_list = []\n",
    "    q_list_l = []\n",
    "    \n",
    "    for t in range(sim_num):\n",
    "        # initialize weight, feature vector, and Q values\n",
    "        init_weight = 1.0\n",
    "        init_feat = []\n",
    "        init_q = []\n",
    "        \n",
    "        for k in range(state_n):\n",
    "            init_feat.append(gamma**(state_n - k - 1)) # feat = [gamma^n-1, gamma^n-2, ..., gamma, 1]\n",
    "        \n",
    "        for k in range(state_n - 1):\n",
    "            init_q.append([gamma**(state_n - k - 1), gamma**(state_n - k - 2)])\n",
    "        \n",
    "        c_weight, c_feat, c_feat_list, c_state_list, c_action_list, c_RPE_list, c_q_RPE_list, \\\n",
    "        c_qraw_RPE_list, c_weight_list, c_epi_num_list, epi_length, q_list = \\\n",
    "        sr_q2(epi_num, gamma, alpha, alpha_sr, state_n, init_feat, feat_list, init_weight,\\\n",
    "              init_q, state_list, action_list, RPE_list, q_RPE_list, qraw_RPE_list, weight_list, epi_num_list, stay_prob, kappa, QorS)\n",
    "        \n",
    "        for u in range(len(epi_length)):\n",
    "            sim_num_list.append(t+1)\n",
    "        \n",
    "        q_list_l.append(q_list)\n",
    "        \n",
    "        feat_list = c_feat_list\n",
    "        state_list = c_state_list\n",
    "        action_list = c_action_list\n",
    "        RPE_list = c_RPE_list\n",
    "        q_RPE_list = c_q_RPE_list\n",
    "        qraw_RPE_list = c_qraw_RPE_list\n",
    "        weight_list = c_weight_list\n",
    "        epi_num_list = c_epi_num_list\n",
    "    \n",
    "    return c_weight, c_feat, c_feat_list, c_state_list, c_action_list, c_RPE_list, c_q_RPE_list, \\\n",
    "c_qraw_RPE_list, c_weight_list, c_epi_num_list, sim_num_list, q_list_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e4f528495325>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m             rl = sr_q3(sim_num, epi_num, gamma, alpha, alpha_sr, state_n, feat_list, \n\u001b[0;32m     29\u001b[0m                           \u001b[0mstate_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRPE_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_RPE_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqraw_RPE_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                           weight_list, epi_num_list, stay_prob, kappa, QorS)\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m# Create dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-602839dcc470>\u001b[0m in \u001b[0;36msr_q3\u001b[1;34m(sim_num, epi_num, gamma, alpha, alpha_sr, state_n, feat_list, state_list, action_list, RPE_list, q_RPE_list, qraw_RPE_list, weight_list, epi_num_list, stay_prob, kappa, QorS)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mc_qraw_RPE_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_weight_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_epi_num_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepi_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_list\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         sr_q2(epi_num, gamma, alpha, alpha_sr, state_n, init_feat, feat_list, init_weight,\\\n\u001b[1;32m--> 153\u001b[1;33m               init_q, state_list, action_list, RPE_list, q_RPE_list, qraw_RPE_list, weight_list, epi_num_list, stay_prob, kappa, QorS)\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepi_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-602839dcc470>\u001b[0m in \u001b[0;36msr_q2\u001b[1;34m(epi_num, gamma, alpha, alpha_sr, state_n, init_feat, feat_list, init_weight, init_q, state_list, action_list, RPE_list, q_RPE_list, qraw_RPE_list, weight_list, epi_num_list, stay_prob, kappa, QorS)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mc_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_state_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_action_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_RPE_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_q_RPE_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_qraw_RPE_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimestep_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_weight_list\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         sr_q1(gamma, alpha, alpha_sr, state_n, init_feat, init_weight, init_q, state_list,\n\u001b[1;32m--> 107\u001b[1;33m               action_list, RPE_list, q_RPE_list, qraw_RPE_list, weight_list, stay_prob, kappa, QorS)\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestep_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-602839dcc470>\u001b[0m in \u001b[0;36msr_q1\u001b[1;34m(gamma, alpha, alpha_sr, state_n, init_feat, init_weight, init_q, state_list, action_list, RPE_list, q_RPE_list, qraw_RPE_list, weight_list, stay_prob, kappa, QorS)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mtime_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRPE_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_RPE_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqraw_RPE_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimestep_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# simulation with different parameters\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "rd.seed(20201105)\n",
    "\n",
    "# Simulate, create dataframe, and save\n",
    "for QorS in [0, 1]:\n",
    "    for kappa in [0.0, 0.20, 0.40]:\n",
    "        for alpha_sr in [0.0, 0.05]:\n",
    "            # set fixed parameters\n",
    "            sim_num = 100\n",
    "            epi_num = 200\n",
    "            gamma = 0.97\n",
    "            alpha = 0.50\n",
    "            state_n = 10\n",
    "            feat_list = []\n",
    "            state_list = []\n",
    "            action_list = []\n",
    "            RPE_list = []\n",
    "            q_RPE_list = []\n",
    "            qraw_RPE_list = []\n",
    "            weight_list = []\n",
    "            stay_prob = 0.75\n",
    "            epi_num_list = []\n",
    "    \n",
    "            # Simulation\n",
    "            rl = sr_q3(sim_num, epi_num, gamma, alpha, alpha_sr, state_n, feat_list, \n",
    "                          state_list, action_list, RPE_list, q_RPE_list, qraw_RPE_list, \n",
    "                          weight_list, epi_num_list, stay_prob, kappa, QorS)\n",
    "    \n",
    "            # Create dataframe\n",
    "            results = \\\n",
    "            pd.DataFrame({'Simulation':rl[10], 'Episode':rl[9], 'State':rl[3], 'Action':rl[4], \n",
    "                          'RPE':rl[5], 'Q_RPE':rl[6], 'Q_RPE_raw':rl[7], 'Weight': rl[8]})\n",
    "\n",
    "            # Convert dataframe to csv\n",
    "            if QorS == 0:\n",
    "                results.to_csv('./SR_flow/Q_alpha_sr{:.0f}_g{:.0f}_s{:.0f}_kappa{:.0f}_{:.0f}states.csv'.format(100*alpha_sr, 100*gamma, 100*stay_prob, 100*kappa, state_n))\n",
    "            \n",
    "            else:\n",
    "                results.to_csv('./SR_flow/SARSA_alpha_sr{:.0f}_g{:.0f}_s{:.0f}_kappa{:.0f}_{:.0f}states.csv'.format(100*alpha_sr, 100*gamma, 100*stay_prob, 100*kappa, state_n))\n",
    "    \n",
    "            # Create dataframe for Q values\n",
    "            q_list = rl[11]\n",
    "            sim_list = []\n",
    "            epi_list = []\n",
    "            state_list = []\n",
    "            q_go = []\n",
    "            q_stay = []\n",
    "\n",
    "            for sim in range(sim_num):\n",
    "                for epi in range(epi_num):\n",
    "                    for state in range(state_n - 1):\n",
    "                        qs = q_list[sim][epi][state]\n",
    "                        sim_list.append(sim+1)\n",
    "                        epi_list.append(epi+1)\n",
    "                        state_list.append(state+1)\n",
    "                        q_go.append(qs[1])\n",
    "                        q_stay.append(qs[0])\n",
    "            \n",
    "            q_values = \\\n",
    "            pd.DataFrame({'Simulation': sim_list, 'Episode': epi_list, 'State': state_list, 'Q_go': q_go, 'Q_stay': q_stay})\n",
    "\n",
    "            # convert dataframe to csv\n",
    "            if QorS == 0:\n",
    "                q_values.to_csv('./SR_flow/Q_Qvalues_alphasr{:.0f}_g{:.0f}_s{:.0f}_kappa{:.0f}_{:.0f}states.csv'.format(100*alpha_sr, 100*gamma, 100*stay_prob, 100*kappa, state_n))\n",
    "            \n",
    "            else:\n",
    "                q_values.to_csv('./SR_flow/SARSA_Qvalues_alphasr{:.0f}_g{:.0f}_s{:.0f}_kappa{:.0f}_{:.0f}states.csv'.format(100*alpha_sr, 100*gamma, 100*stay_prob, 100*kappa, state_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
